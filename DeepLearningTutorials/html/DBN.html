<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Deep Belief Networks &mdash; DeepLearning v0.1 documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '#',
        VERSION:     '0.1',
        COLLAPSE_MODINDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="DeepLearning v0.1 documentation" href="index.html" />
    <link rel="next" title="Miscellaneous" href="utilities.html" />
    <link rel="prev" title="Restricted Boltzmann Machines (RBM)" href="rbm.html" />
 
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-168290-9']);
  _gaq.push(['_trackPageview']);
</script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="utilities.html" title="Miscellaneous"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="rbm.html" title="Restricted Boltzmann Machines (RBM)"
             accesskey="P">previous</a> |</li>
        <li><a href="contents.html">DeepLearning v0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <h3><a href="contents.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference external" href="#">Deep Belief Networks</a><ul>
<li><a class="reference external" href="#id1">Deep Belief Networks</a></li>
<li><a class="reference external" href="#justifying-greedy-layer-wise-pre-training">Justifying Greedy-Layer Wise Pre-Training</a></li>
<li><a class="reference external" href="#implementation">Implementation</a></li>
<li><a class="reference external" href="#putting-it-all-together">Putting it all together</a></li>
<li><a class="reference external" href="#running-the-code">Running the Code</a></li>
<li><a class="reference external" href="#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="rbm.html"
                                  title="previous chapter">Restricted Boltzmann Machines (RBM)</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="utilities.html"
                                  title="next chapter">Miscellaneous</a></p>
            <h3>This Page</h3>
            <ul class="this-page-menu">
              <li><a href="_sources/DBN.txt"
                     rel="nofollow">Show Source</a></li>
            </ul>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="deep-belief-networks">
<span id="dbn"></span><h1>Deep Belief Networks<a class="headerlink" href="#deep-belief-networks" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This section assumes the reader has already read through <a class="reference external" href="logreg.html"><em>Classifying MNIST digits using Logistic Regression</em></a>
and <a class="reference external" href="mlp.html"><em>Multilayer Perceptron</em></a> and <a class="reference external" href="rbm.html"><em>Restricted Boltzmann Machines (RBM)</em></a>. Additionally it uses the following Theano
functions and concepts : <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/examples.html?highlight=tanh">T.tanh</a>, <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables">shared variables</a>, <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/adding.html#adding-two-scalars">basic arithmetic
ops</a>, <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/examples.html#computing-gradients">T.grad</a>, <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/examples.html#using-random-numbers">Random numbers</a>, <a class="reference external" href="http://deeplearning.net/software/theano/library/config.html#config.floatX">floatX</a>. If you intend to run the
code on GPU also read <a class="reference external" href="http://deeplearning.net/software/theano/tutorial/using_gpu.html">GPU</a>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The code for this section is available for download <a class="reference external" href="http://deeplearning.net/tutorial/code/DBN.py">here</a>.</p>
</div>
<div class="section" id="id1">
<h2>Deep Belief Networks<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="references.html#hinton06">[Hinton06]</a> showed that RBMs can be stacked and trained in a greedy manner
to form so-called Deep Belief Networks (DBN). DBNs are graphical models which
learn to extract a deep hierarchical representation of the training data.
They model the joint distribution between observed vector <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> and
the <img class="math" src="_images/math/63c17c295325f731666c7d74952b563a01e00fcc.png" alt="\ell"/> hidden layers <img class="math" src="_images/math/be8ad325b1eeb5121eb2027f11c8e95260d68540.png" alt="h^k"/> as follows:</p>
<div class="math" id="equation-dbn">
<p><span class="eqno">(1)</span><img src="_images/math/0440dbdcf20bebd73fa68d3e38adcb1e945d52f2.png" alt="P(x, h^1, \ldots, h^{\ell}) = \left(\prod_{k=0}^{\ell-2} P(h^k|h^{k+1})\right) P(h^{\ell-1},h^{\ell})" /></p>
</div><p>where <img class="math" src="_images/math/4e0d9fb02137134bf02b39aafc3c422fa7cb9d3a.png" alt="x=h^0"/>, <img class="math" src="_images/math/c0e359a5ddc898306619dc0a4974ff20ad3bcc75.png" alt="P(h^{k-1} | h^k)"/> is a conditional distribution
for the visible units conditioned on the hidden units of the RBM at level
<img class="math" src="_images/math/8c325612684d41304b9751c175df7bcc0f61f64f.png" alt="k"/>, and <img class="math" src="_images/math/03b1cdeb23b8b45e1878a38afa3b4a60ebd93bcd.png" alt="P(h^{\ell-1}, h^{\ell})"/> is the visible-hidden joint
distribution in the top-level RBM. This is illustrated in the figure below.</p>
<div class="figure align-center">
<img alt="_images/DBN3.png" src="_images/DBN3.png" />
</div>
<p>The principle of greedy layer-wise unsupervised training can be applied to
DBNs with RBMs as the building blocks for each layer <a class="reference external" href="references.html#hinton06">[Hinton06]</a>, <a class="reference external" href="references.html#bengio07">[Bengio07]</a>.
The process is as follows:</p>
<p>1. Train the first layer as an RBM that models the raw input <img class="math" src="_images/math/950b83a1cd25ba458e5873a34612f07e30df3908.png" alt="x =
h^{(0)}"/> as its visible layer.</p>
<p>2. Use that first layer to obtain a representation of the input that will
be used as data for the second layer. Two common solutions exist. This
representation can be chosen as being the mean activations
<img class="math" src="_images/math/e0bdae35d9c0c444a406f4c9be29c033317fbec0.png" alt="p(h^{(1)}=1|h^{(0)})"/> or samples of <img class="math" src="_images/math/dfae483dea1b6dbd48905b549d597ae84ff4977b.png" alt="p(h^{(1)}|h^{(0)})"/>.</p>
<p>3. Train the second layer as an RBM, taking the transformed data (samples or
mean activations) as training examples (for the visible layer of that RBM).</p>
<p>4. Iterate (2 and 3) for the desired number of layers, each time propagating
upward either samples or mean values.</p>
<p>5. Fine-tune all the parameters of this deep architecture with respect to a
proxy for the DBN log- likelihood, or with respect to a supervised training
criterion (after adding extra learning machinery to convert the learned
representation into supervised predictions, e.g. a linear classifier).</p>
<p>In this tutorial, we focus on fine-tuning via supervised gradient descent.
Specifically, we use a logistic regression classifier to classify the input
<img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> based on the output of the last hidden layer <img class="math" src="_images/math/fe87df1809163b2d66595be262741647f9e1376d.png" alt="h^{(l)}"/> of the
DBN. Fine-tuning is then performed via supervised gradient descent of the
negative log-likelihood cost function. Since the supervised gradient is only
non-null for the weights and hidden layer biases of each layer (i.e. null for
the visible biases of each RBM), this procedure is equivalent to initializing
the parameters of a deep MLP with the weights and hidden layer biases obtained
with the unsupervised training strategy.</p>
</div>
<div class="section" id="justifying-greedy-layer-wise-pre-training">
<h2>Justifying Greedy-Layer Wise Pre-Training<a class="headerlink" href="#justifying-greedy-layer-wise-pre-training" title="Permalink to this headline">¶</a></h2>
<p>Why does such an algorithm work ? Taking as example a 2-layer DBN with hidden
layers <img class="math" src="_images/math/c0ed0f5786205f4105cdf86fcd5d7e8df4dc137b.png" alt="h^{(1)}"/> and <img class="math" src="_images/math/163e8ffeb002cc55f4d93cf5988f18048e008da0.png" alt="h^{(2)}"/> (with respective weight parameters
<img class="math" src="_images/math/1eaa9c47d468ebb6f7c89c272d28302cf1d6e5c4.png" alt="W^{(1)}"/> and <img class="math" src="_images/math/9a26b5f3f35aee32d343345552ce0dc19c229826.png" alt="W^{(2)}"/>), <a class="reference external" href="references.html#hinton06">[Hinton06]</a> established
(see also Bengio09]_ for a detailed derivation) that <img class="math" src="_images/math/462dafcf4b9d52451bc40f3bedd8bb01030d4e3a.png" alt="\log
p(x)"/> can be rewritten as,</p>
<div class="math" id="equation-dbn_bound">
<p><span class="eqno">(2)</span><img src="_images/math/5f989a71a401dcb525a81b445b0f7fcc4ab6d183.png" alt="\log p(x) = &amp;KL(Q(h^{(1)}|x)||p(h^{(1)}|x)) + H_{Q(h^{(1)}|x)} + \\
            &amp;\sum_h Q(h^{(1)}|x)(\log p(h^{(1)}) + \log p(x|h^{(1)}))." /></p>
</div><p><img class="math" src="_images/math/b66591b680eb53201ac1b1907394c100dd6b68f1.png" alt="KL(Q(h^{(1)}|x) || p(h^{(1)}|x))"/> represents the KL divergence between
the posterior <img class="math" src="_images/math/9f571c610d65042933b15bc838862e0985c20fb8.png" alt="Q(h^{(1)}|x)"/> of the first RBM if it were standalone, and the
probability <img class="math" src="_images/math/e782aa3166689d5832838b80daee2d290032afbe.png" alt="p(h^{(1)}|x)"/> for the same layer but defined by the entire DBN
(i.e. taking into account the prior <img class="math" src="_images/math/62de76702cb2882395603f08ec97fd87fd08b997.png" alt="p(h^{(1)},h^{(2)})"/> defined by the
top-level RBM). <img class="math" src="_images/math/8ee4b6b0c41b635722dec68ca9fa17e635d2d820.png" alt="H_{Q(h^{(1)}|x)}"/> is the entropy of the distribution
<img class="math" src="_images/math/9f571c610d65042933b15bc838862e0985c20fb8.png" alt="Q(h^{(1)}|x)"/>.</p>
<p>It can be shown that if we initialize both hidden layers such that
<img class="math" src="_images/math/49fb25c40890086c2e73942d12e066756085423e.png" alt="W^{(2)}={W^{(1)}}^T"/>, <img class="math" src="_images/math/a97f2281b6a6640ba6c411fccd9066f6f3c0eccc.png" alt="Q(h^{(1)}|x)=p(h^{(1)}|x)"/> and the KL
divergence term is null. If we learn the first level RBM and then keep its
parameters <img class="math" src="_images/math/1eaa9c47d468ebb6f7c89c272d28302cf1d6e5c4.png" alt="W^{(1)}"/> fixed, optimizing Eq. <a href="#equation-dbn_bound">(2)</a> with respect
to <img class="math" src="_images/math/9a26b5f3f35aee32d343345552ce0dc19c229826.png" alt="W^{(2)}"/> can thus only increase the likelihood <img class="math" src="_images/math/2751d79d3bbfb34440d68c685fe6ba7414951749.png" alt="p(x)"/>.</p>
<p>Also, notice that if we isolate the terms which depend only on <img class="math" src="_images/math/9a26b5f3f35aee32d343345552ce0dc19c229826.png" alt="W^{(2)}"/>, we
get:</p>
<div class="math">
<p><img src="_images/math/a1eff84181b683dd645fcbedec99d488936b9795.png" alt="\sum_h Q(h^{(1)}|x)p(h^{(1)})" /></p>
</div><p>Optimizing this with respect to <img class="math" src="_images/math/9a26b5f3f35aee32d343345552ce0dc19c229826.png" alt="W^{(2)}"/> amounts to training a second-stage
RBM, using the output of <img class="math" src="_images/math/9f571c610d65042933b15bc838862e0985c20fb8.png" alt="Q(h^{(1)}|x)"/> as the training distribution,
when <img class="math" src="_images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> is sampled from the training distribution for the first RBM.</p>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<p>To implement DBNs in Theano, we will use the class defined in the <a class="reference external" href="rbm.html"><em>Restricted Boltzmann Machines (RBM)</em></a>
tutorial. One can also observe that the code for the DBN is very similar with the one
for SdA, because both involve the principle of unsupervised layer-wise
pre-training followed by supervised fine-tuning as a deep MLP.
The main difference is that we use the RBM class instead of the dA
class.</p>
<p>We start off by defining the DBN class which will store the layers of the
MLP, along with their associated RBMs. Since we take the viewpoint of using
the RBMs to initialize an MLP, the code will reflect this by seperating as
much as possible the RBMs used to initialize the network and the MLP used for
classification.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">DBN</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numpy_rng</span><span class="p">,</span> <span class="n">theano_rng</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">n_ins</span> <span class="o">=</span> <span class="mi">784</span><span class="p">,</span>
                <span class="n">hidden_layers_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">500</span><span class="p">],</span> <span class="n">n_outs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;This class is made to support a variable number of layers.</span>

<span class="sd">        :type numpy_rng: numpy.random.RandomState</span>
<span class="sd">        :param numpy_rng: numpy random number generator used to draw initial</span>
<span class="sd">                weights</span>

<span class="sd">        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams</span>
<span class="sd">        :param theano_rng: Theano random generator; if None is given one is</span>
<span class="sd">                       generated based on a seed drawn from `rng`</span>

<span class="sd">        :type n_ins: int</span>
<span class="sd">        :param n_ins: dimension of the input to the DBN</span>

<span class="sd">        :type n_layers_sizes: list of ints</span>
<span class="sd">        :param n_layers_sizes: intermediate layers size, must contain</span>
<span class="sd">                           at least one value</span>

<span class="sd">        :type n_outs: int</span>
<span class="sd">        :param n_outs: dimension of the output of the network</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rbm_layers</span>     <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span>         <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span>       <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_layers_sizes</span><span class="p">)</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">theano_rng</span><span class="p">:</span>
            <span class="n">theano_rng</span> <span class="o">=</span> <span class="n">RandomStreams</span><span class="p">(</span><span class="n">numpy_rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="mi">30</span><span class="p">))</span>

        <span class="c"># allocate symbolic variables for the data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span>  <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="s">&#39;x&#39;</span><span class="p">)</span>  <span class="c"># the data is presented as rasterized images</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span>  <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">ivector</span><span class="p">(</span><span class="s">&#39;y&#39;</span><span class="p">)</span> <span class="c"># the labels are presented as 1D vector of</span>
                                 <span class="c"># [int] labels</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">self.sigmoid_layers</span></tt> will store the feed-forward graphs which together form
the MLP, while <tt class="docutils literal"><span class="pre">self.rbm_layers</span></tt> will store the RBMs used to pretrain each
layer of the MLP.</p>
<p>Next step, we construct <tt class="docutils literal"><span class="pre">n_layers</span></tt> sigmoid layers (we use the
<tt class="docutils literal"><span class="pre">SigmoidalLayer</span></tt> class introduced in <a class="reference external" href="mlp.html#mlp"><em>Multilayer Perceptron</em></a>, with the only modification
that we replaced the non-linearity from <tt class="docutils literal"><span class="pre">tanh</span></tt> to the logistic function
<img class="math" src="_images/math/5724261979966cc0a9071a92ce39f4aa238e101f.png" alt="s(x) = \frac{1}{1+e^{-x}}"/>) and <tt class="docutils literal"><span class="pre">n_layers</span></tt> RBMs, where <tt class="docutils literal"><span class="pre">n_layers</span></tt>
is the depth of our model.  We link the sigmoid layers such that they form an
MLP, and construct each RBM such that they share the weight matrix and the
hidden bias with its corresponding sigmoid layer.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="p">):</span>
    <span class="c"># construct the sigmoidal layer</span>

    <span class="c"># the size of the input is either the number of hidden units of the layer below or</span>
    <span class="c"># the input size if we are on the first layer</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">n_ins</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">hidden_layers_sizes</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># the input to this layer is either the activation of the hidden layer below or the</span>
    <span class="c"># input of the DBN if you are on the first layer</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
        <span class="n">layer_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">layer_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span>

    <span class="n">sigmoid_layer</span> <span class="o">=</span> <span class="n">HiddenLayer</span><span class="p">(</span><span class="n">rng</span>   <span class="o">=</span> <span class="n">numpy_rng</span><span class="p">,</span>
                                <span class="nb">input</span> <span class="o">=</span> <span class="n">layer_input</span><span class="p">,</span>
                                <span class="n">n_in</span>  <span class="o">=</span> <span class="n">input_size</span><span class="p">,</span>
                                <span class="n">n_out</span> <span class="o">=</span> <span class="n">hidden_layers_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                <span class="n">activation</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">)</span>

    <span class="c"># add the layer to our list of layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigmoid_layer</span><span class="p">)</span>

    <span class="c"># its arguably a philosophical question...  but we are going to only declare that</span>
    <span class="c"># the parameters of the sigmoid_layers are parameters of the DBN. The visible</span>
    <span class="c"># biases in the RBM are parameters of those RBMs, but not of the DBN.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">sigmoid_layer</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="c"># Construct an RBM that shared weights with this layer</span>
    <span class="n">rbm_layer</span> <span class="o">=</span> <span class="n">RBM</span><span class="p">(</span><span class="n">numpy_rng</span> <span class="o">=</span> <span class="n">numpy_rng</span><span class="p">,</span> <span class="n">theano_rng</span> <span class="o">=</span> <span class="n">theano_rng</span><span class="p">,</span>
                    <span class="nb">input</span> <span class="o">=</span> <span class="n">layer_input</span><span class="p">,</span>
                    <span class="n">n_visible</span> <span class="o">=</span> <span class="n">input_size</span><span class="p">,</span>
                    <span class="n">n_hidden</span>  <span class="o">=</span> <span class="n">hidden_layers_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">W</span> <span class="o">=</span> <span class="n">sigmoid_layer</span><span class="o">.</span><span class="n">W</span><span class="p">,</span>
                    <span class="n">hbias</span> <span class="o">=</span> <span class="n">sigmoid_layer</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rbm_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rbm_layer</span><span class="p">)</span>
</pre></div>
</div>
<p>All that is left is to stack one last logistic regression layer in order to
form an MLP. We will use the <tt class="docutils literal"><span class="pre">LogisticRegression</span></tt> class introduced in
<a class="reference external" href="logreg.html#logreg"><em>Classifying MNIST digits using Logistic Regression</em></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># We now need to add a logistic layer on top of the MLP</span>
<span class="bp">self</span><span class="o">.</span><span class="n">logLayer</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span>\
                 <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">,</span>\
                 <span class="n">n_in</span> <span class="o">=</span> <span class="n">hidden_layers_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_out</span> <span class="o">=</span> <span class="n">n_outs</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logLayer</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

<span class="c"># construct a function that implements one step of fine-tuning compute the cost for</span>
<span class="c"># second phase of training, defined as the negative log likelihood</span>
<span class="c"># of the logistic regression (output) layer</span>
<span class="bp">self</span><span class="o">.</span><span class="n">finetune_cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logLayer</span><span class="o">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>

<span class="c"># compute the gradients with respect to the model parameters</span>
<span class="c"># symbolic variable that points to the number of errors made on the</span>
<span class="c"># minibatch given by self.x and self.y</span>
<span class="bp">self</span><span class="o">.</span><span class="n">errors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logLayer</span><span class="o">.</span><span class="n">errors</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>The class also provides a method which generates training functions for each
of the RBMs. They are returned as a list, where element <img class="math" src="_images/math/34857b3ba74ce5cd8607f3ebd23e9015908ada71.png" alt="i"/> is a
function which implements one step of training for the <tt class="docutils literal"><span class="pre">RBM</span></tt> at layer
<img class="math" src="_images/math/34857b3ba74ce5cd8607f3ebd23e9015908ada71.png" alt="i"/>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">pretraining_functions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_set_x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39; Generates a list of functions, for performing one step of gradient descent at a</span>
<span class="sd">    given layer. The function will require as input the minibatch index, and to train an</span>
<span class="sd">    RBM you just need to iterate, calling the corresponding function on all minibatch</span>
<span class="sd">    indexes.</span>

<span class="sd">    :type train_set_x: theano.tensor.TensorType</span>
<span class="sd">    :param train_set_x: Shared var. that contains all datapoints used for training the RBM</span>
<span class="sd">    :type batch_size: int</span>
<span class="sd">    :param batch_size: size of a [mini]batch</span>
<span class="sd">    :param k: number of Gibbs steps to do in CD-k / PCD-k</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="c"># index to a [mini]batch</span>
    <span class="n">index</span>            <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">(</span><span class="s">&#39;index&#39;</span><span class="p">)</span>   <span class="c"># index to a minibatch</span>
</pre></div>
</div>
<p>In order to be able to change the learning rate during training, we associate a
Theano variable to it that has a default value.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">learning_rate</span>    <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">&#39;lr&#39;</span><span class="p">)</span>    <span class="c"># learning rate to use</span>

<span class="c"># number of batches</span>
<span class="n">n_batches</span> <span class="o">=</span> <span class="n">train_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span>
<span class="c"># begining of a batch, given `index`</span>
<span class="n">batch_begin</span> <span class="o">=</span> <span class="n">index</span> <span class="o">*</span> <span class="n">batch_size</span>
<span class="c"># ending of a batch given `index`</span>
<span class="n">batch_end</span> <span class="o">=</span> <span class="n">batch_begin</span><span class="o">+</span><span class="n">batch_size</span>

<span class="n">pretrain_fns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">rbm</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">rbm_layers</span><span class="p">:</span>

    <span class="c"># get the cost and the updates list</span>
    <span class="c"># using CD-k here (persisent=None) for training each RBM.</span>
    <span class="c"># TODO: change cost function to reconstruction error</span>
    <span class="n">cost</span><span class="p">,</span><span class="n">updates</span> <span class="o">=</span> <span class="n">rbm</span><span class="o">.</span><span class="n">cd</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

    <span class="c"># compile the Theano function; check if k is also a Theano</span>
    <span class="c"># variable, if so added to the inputs of the function</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">theano</span><span class="o">.</span><span class="n">Variable</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span> <span class="n">index</span><span class="p">,</span> <span class="n">theano</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span><span class="n">k</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span> <span class="n">index</span><span class="p">,</span> <span class="n">theano</span><span class="o">.</span><span class="n">Param</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)]</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">,</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">cost</span><span class="p">,</span>
            <span class="n">updates</span> <span class="o">=</span> <span class="n">updates</span><span class="p">,</span>
            <span class="n">givens</span>  <span class="o">=</span> <span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="p">:</span><span class="n">train_set_x</span><span class="p">[</span><span class="n">batch_begin</span><span class="p">:</span><span class="n">batch_end</span><span class="p">]})</span>
    <span class="c"># append `fn` to the list of functions</span>
    <span class="n">pretrain_fns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

<span class="k">return</span> <span class="n">pretrain_fns</span>
</pre></div>
</div>
<p>Now any function <tt class="docutils literal"><span class="pre">pretrain_fns[i]</span></tt> takes as arguments <tt class="docutils literal"><span class="pre">index</span></tt> and
optionally <tt class="docutils literal"><span class="pre">lr</span></tt> &#8211; the learning rate. Note that the names of the parameters
are the names given to the Theano variables (e.g. <tt class="docutils literal"><span class="pre">lr</span></tt>) when they are
constructed and not the name of the python variables (e.g. <tt class="docutils literal"><span class="pre">learning_rate</span></tt>). Keep
this in mind when working with Theano. Optionally, if you provide <tt class="docutils literal"><span class="pre">k</span></tt> (the
number of Gibbs steps to perform in CD or PCD) this will also become an
argument of your function.</p>
<p>In the same fashion, the DBN class includes a method for building the
functions required for finetuning ( a <tt class="docutils literal"><span class="pre">train_model</span></tt>, a <tt class="docutils literal"><span class="pre">validate_model</span></tt>
and a <tt class="docutils literal"><span class="pre">test_model</span></tt> function).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">build_finetune_functions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;Generates a function `train` that implements one step of finetuning, a function</span>
<span class="sd">    `validate` that computes the error on a batch from the validation set, and a function</span>
<span class="sd">    `test` that computes the error on a batch from the testing set</span>

<span class="sd">    :type datasets: list of pairs of theano.tensor.TensorType</span>
<span class="sd">    :param datasets: It is a list that contain all the datasets;  the has to contain three</span>
<span class="sd">    pairs, `train`, `valid`, `test` in this order, where each pair is formed of two Theano</span>
<span class="sd">    variables, one for the datapoints, the other for the labels</span>
<span class="sd">    :type batch_size: int</span>
<span class="sd">    :param batch_size: size of a minibatch</span>
<span class="sd">    :type learning_rate: float</span>
<span class="sd">    :param learning_rate: learning rate used during finetune stage</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="p">(</span><span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">(</span><span class="n">valid_set_x</span><span class="p">,</span> <span class="n">valid_set_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">(</span><span class="n">test_set_x</span> <span class="p">,</span> <span class="n">test_set_y</span> <span class="p">)</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="c"># compute number of minibatches for training, validation and testing</span>
    <span class="n">n_valid_batches</span> <span class="o">=</span> <span class="n">valid_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span>
    <span class="n">n_test_batches</span>  <span class="o">=</span> <span class="n">test_set_x</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="n">borrow</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="o">/</span> <span class="n">batch_size</span>

    <span class="n">index</span>   <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">lscalar</span><span class="p">(</span><span class="s">&#39;index&#39;</span><span class="p">)</span>    <span class="c"># index to a [mini]batch</span>

    <span class="c"># compute the gradients with respect to the model parameters</span>
    <span class="n">gparams</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">finetune_cost</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="c"># compute list of fine-tuning updates</span>
    <span class="n">updates</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">param</span><span class="p">,</span> <span class="n">gparam</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">gparams</span><span class="p">):</span>
        <span class="n">updates</span><span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">=</span> <span class="n">param</span> <span class="o">-</span> <span class="n">gparam</span><span class="o">*</span><span class="n">learning_rate</span>

    <span class="n">train_fn</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">index</span><span class="p">],</span>
          <span class="n">outputs</span> <span class="o">=</span>   <span class="bp">self</span><span class="o">.</span><span class="n">finetune_cost</span><span class="p">,</span>
          <span class="n">updates</span> <span class="o">=</span> <span class="n">updates</span><span class="p">,</span>
          <span class="n">givens</span>  <span class="o">=</span> <span class="p">{</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="p">:</span> <span class="n">train_set_x</span><span class="p">[</span><span class="n">index</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="p">:</span> <span class="n">train_set_y</span><span class="p">[</span><span class="n">index</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]})</span>

    <span class="n">test_score_i</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="p">,</span>
             <span class="n">givens</span> <span class="o">=</span> <span class="p">{</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">:</span> <span class="n">test_set_x</span><span class="p">[</span><span class="n">index</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">],</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">:</span> <span class="n">test_set_y</span><span class="p">[</span><span class="n">index</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]})</span>

    <span class="n">valid_score_i</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="p">,</span>
          <span class="n">givens</span> <span class="o">=</span> <span class="p">{</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">:</span> <span class="n">valid_set_x</span><span class="p">[</span><span class="n">index</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">],</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">:</span> <span class="n">valid_set_y</span><span class="p">[</span><span class="n">index</span><span class="o">*</span><span class="n">batch_size</span><span class="p">:(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]})</span>

    <span class="c"># Create a function that scans the entire validation set</span>
    <span class="k">def</span> <span class="nf">valid_score</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">valid_score_i</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_valid_batches</span><span class="p">)]</span>

    <span class="c"># Create a function that scans the entire test set</span>
    <span class="k">def</span> <span class="nf">test_score</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">test_score_i</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_test_batches</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">train_fn</span><span class="p">,</span> <span class="n">valid_score</span><span class="p">,</span> <span class="n">test_score</span>
</pre></div>
</div>
<p>Note that the returned <tt class="docutils literal"><span class="pre">valid_score</span></tt> and <tt class="docutils literal"><span class="pre">test_score</span></tt> are not Theano
functions, but rather Python functions. These loop over the entire
validation set and the entire test set to produce a list of the losses
obtained over these sets.</p>
</div>
<div class="section" id="putting-it-all-together">
<h2>Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this headline">¶</a></h2>
<p>The few lines of code below constructs the deep belief network :</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">numpy_rng</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="k">print</span> <span class="s">&#39;... building the model&#39;</span>
<span class="c"># construct the Deep Belief Network</span>
<span class="n">dbn</span> <span class="o">=</span> <span class="n">DBN</span><span class="p">(</span><span class="n">numpy_rng</span> <span class="o">=</span> <span class="n">numpy_rng</span><span class="p">,</span> <span class="n">n_ins</span> <span class="o">=</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span>
          <span class="n">hidden_layers_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1000</span><span class="p">],</span>
          <span class="n">n_outs</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>There are two stages in training this network: (1) a layer-wise pre-training and
(2) a fine-tuning stage.</p>
<p>For the pre-training stage, we loop over all the layers of the network. For
each layer, we use the compiled theano function which determines the
input to the <tt class="docutils literal"><span class="pre">i</span></tt>-th level RBM and performs one step of CD-k within this RBM.
This function is applied to the training set for a fixed number of epochs
given by <tt class="docutils literal"><span class="pre">pretraining_epochs</span></tt>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">#########################</span>
<span class="c"># PRETRAINING THE MODEL #</span>
<span class="c">#########################</span>
<span class="k">print</span> <span class="s">&#39;... getting the pretraining functions&#39;</span>
<span class="c"># We are using CD-1 here</span>
<span class="n">pretraining_fns</span> <span class="o">=</span> <span class="n">dbn</span><span class="o">.</span><span class="n">pretraining_functions</span><span class="p">(</span>
        <span class="n">train_set_x</span>   <span class="o">=</span> <span class="n">train_set_x</span><span class="p">,</span>
        <span class="n">batch_size</span>    <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span>
        <span class="n">k</span>             <span class="o">=</span> <span class="n">k</span><span class="p">)</span>

<span class="k">print</span> <span class="s">&#39;... pre-training the model&#39;</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">clock</span><span class="p">()</span>
<span class="c">## Pre-train layer-wise</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">dbn</span><span class="o">.</span><span class="n">n_layers</span><span class="p">):</span>
    <span class="c"># go through pretraining epochs</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">pretraining_epochs</span><span class="p">):</span>
        <span class="c"># go through the training set</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">n_train_batches</span><span class="p">):</span>
            <span class="n">c</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pretraining_fns</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">index</span> <span class="o">=</span> <span class="n">batch_index</span><span class="p">,</span>
                     <span class="n">lr</span> <span class="o">=</span> <span class="n">pretrain_lr</span> <span class="p">)</span> <span class="p">)</span>
        <span class="k">print</span> <span class="s">&#39;Pre-training layer </span><span class="si">%i</span><span class="s">, epoch </span><span class="si">%d</span><span class="s">, cost &#39;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">epoch</span><span class="p">),</span><span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">clock</span><span class="p">()</span>
</pre></div>
</div>
<p>The fine-tuning loop is very similar to the one in the <a class="reference external" href="mlp.html#mlp"><em>Multilayer Perceptron</em></a> tutorial,
the only difference being that we now use the functions given by
<img class="math" src="_images/math/1735cdfbfbfc569c028c02ee182eb0c6b1e00475.png" alt="build_finetune_functions"/>.</p>
</div>
<div class="section" id="running-the-code">
<h2>Running the Code<a class="headerlink" href="#running-the-code" title="Permalink to this headline">¶</a></h2>
<p>The user can run the code by calling:</p>
<div class="highlight-bash"><div class="highlight"><pre>python code/DBN.py
</pre></div>
</div>
<p>With the default parameters, the code runs for 100 pre-training epochs with
mini-batches of size 10. This corresponds to performing 500,000 unsupervised
parameter updates. We use an unsupervised learning rate of 0.01, with a
supervised learning rate of 0.1.  The DBN itself consists of three
hidden layers with 1000 units per layer. With early-stopping, this configuration
achieved a minimal validation error of 1.27 with corresponding test
error of 1.34 after 46 supervised epochs.</p>
<p>On an Intel(R) Xeon(R) CPU X5560 running at 2.80GHz, using a multi-threaded MKL
library (running on 4 cores), pretraining took 615 minutes with an average of
2.05 mins/(layer * epoch). Fine-tuning took only 101 minutes or approximately
2.20 mins/epoch.</p>
<p>Hyper-parameters were selected by optimizing on the validation error. We tested
unsupervised learning rates in <img class="math" src="_images/math/cd135f7eead0d9a3d38bdebb4084b8e3d9de87f2.png" alt="\{10^{-1}, ..., 10^{-5}\}"/> and supervised
learning rates in <img class="math" src="_images/math/15a411b25f834fbcbece9c525a5c30d675898c70.png" alt="\{10^{-1}, ..., 10^{-4}\}"/>. We did not use any form of
regularization besides early-stopping, nor did we optimize over the number of
pretraining updates.</p>
</div>
<div class="section" id="tips-and-tricks">
<h2>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Permalink to this headline">¶</a></h2>
<p>One way to improve the running time of your code (given that you have
sufficient memory available), is to compute the representation of the entire
dataset at layer <tt class="docutils literal"><span class="pre">i</span></tt> in a single pass, once the weights of the
<img class="math" src="_images/math/27571bcad594106e5d9a023867301830b58f569e.png" alt="i-1"/>-th layers have been fixed. Namely, start by training your first
layer RBM. Once it is trained, you can compute the hidden units values for
every example in the dataset and store this as a new dataset which is used to
train the 2nd layer RBM. Once you trained the RBM for layer 2, you compute, in
a similar fashion, the dataset for layer 3 and so on. This avoids calculating
the intermediate (hidden layer) representations, <tt class="docutils literal"><span class="pre">pretraining_epochs</span></tt> times
at the expense of increased memory usage.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="utilities.html" title="Miscellaneous"
             >next</a> |</li>
        <li class="right" >
          <a href="rbm.html" title="Restricted Boltzmann Machines (RBM)"
             >previous</a> |</li>
        <li><a href="contents.html">DeepLearning v0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>

    <div class="footer">
      &copy; Copyright 2008--2010, LISA lab.
      Last updated on Oct 05, 2011.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 0.6.5.
    </div>
<script type="text/javascript">
  (function() {
    var ga = document.createElement('script');
    ga.src = ('https:' == document.location.protocol ?
              'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    ga.setAttribute('async', 'true');
    document.documentElement.firstChild.appendChild(ga);
  })();
</script>

  </body>
</html>